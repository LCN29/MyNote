# 多线程性能调校

#### 内部锁的优化

(1)锁消除
　　锁消除(Lock Elision)是JIT编译器对内部锁的具体实现所做的一种优化。 在动态编译同步块的时候, JIT编译器可以借助一种被称为逃逸分析(Escape Analysis)的技术来判断同步块所使用的锁对象是否只能够被一个线程访问而没有被发布到其他线程。 如果同步块所使用的锁对象通过这种分析被证实只能够被一个线程访问, 那么JIT编译器在编译这个同步块的时候并不生成synchronized所表示的锁的申请与释放对应的机器码, 而仅生成原临界区代码对应的机器码, 这就造成了被动态编译的字节码就像是不包含monitorenter(申请锁)和monitorexit(释放锁)这两个字节码指令一样, 即消除了锁的使用。 这种编译器优化就被称为锁消除(Lock Elision), 它使得特定情况下我们可以完全消除锁的开销。 
```java
	
	// 待编译的代码  有且只要一个线程会执行这段代码
	synchronized(monitor) {
		doSomethings();
	}

	// 优化后的代码
	doSomethings();
```
　　Java标准库中的有些类(比如StringBuffer)虽然是线程安全的, 但是在实际使用中我们往往不在多个线程间共享这些类的实例。 而这些类在实现线程安全的时候往往借助于内部锁。 因此, 这些类是锁消除优化的常见目标。 
```java

	public static String toJson(ProductInfo info) {

		/**
		 * JIT编译器在编译toJSON方法的时候会将其调用的StringBuffer.append/toString方法内联(Inline)到该方法之中, 
		 * 这相当于把StringBuffer.append/toString方法的方法体中的指令复制到toJSON方法体之中。 (既把其代码拷贝到方法里面)	
		 * 此时的sb是一个局部变量，并且该变量所引用的对象并没有被发布到其他线程, 
		 * 因此sbf引用的对象只能够被sb所在的方法(toJSON方法)的当前执行线程(一个线程)访问。 
		 * 所以, JIT编译器此时可以消除toJSON方法中从StringBuffer.append/toString方法的方法体复制的指令所使用的内部锁。 
		 */
		StringBuffer sb = new StringBuffer();
		sb.append("{\"productID\":\"").append(info.getProductId());
		sb.append(",\"inventory\":").append(productInfo.inventory);
		sb.append("\"}");
		return sb.toString();
	}
```
　　从上述例子可以看出, 锁消除优化还可能需要以JIT编译器的内联优化为前提。 而一个方法是否会被JIT编译器内联取决于该方法的热度以及该方法对应的字节码的尺寸(BytecodeSize)。 因此, 锁消除优化能否被实施还取决于被调用的同步方法(或者带同步块的方法)是否能够被内联。 锁消除优化能否被实施还取决于被调用的同步方法(或者带同步块的方法)是否能够被内联。 

　　锁消除优化并不表示开发人员在编写代码的时候可以随意使用内部锁(在不需要加锁的情况下加锁), 因为锁消除是JIT编译器而不是javac所做的一种优化, 而一段代码只有在其被执行的频率足够大的情况下才有可能会被JIT编译器优化。 也就是说在JIT编译器优化介入之前, 只要源代码中使用了内部锁, 那么这个锁的开销就会存在。 另外, JIT编译器所执行的内联优化、逃逸分析以及锁消除优化本身都是有其开销的。 


(2)锁粗化
　　锁粗化(Lock Coarsening/Lock Merging)是JIT编译器对内部锁的具体实现所做的一种优化, 对于相邻的几个同步块, 如果这些同步块使用的是同一个锁实例, 那么JIT编译器会将这些同步块合并为一个大同步块, 从而避免了一个线程反复申请、释放同一个锁所导致的开销。 然而, 锁粗化可能导致一个线程持续持有一个锁的时间变长, 从而使得同步在该锁之上的其他线程在申请锁时的等待时间变长。 
```java
	synchronized(monitor) {
		doSomethings1();
	}
	// 此处, 原本是可以被其他线程争夺，获取锁monitor的
	synchronized(monitor) {
		doSomethings2();
	}

	synchronized(monitor) {
		doSomethings3();
	}

	//优化后
	synchronized(monitor) {
		doSomethings1();
		doSomethings2();
		doSomethings3();
	}
```
　　相邻的两个同步块之间如果存在其他语句, 也不一定就会阻碍JIT编译器执行锁粗化优化, 这是因为JIT编译器可能在执行锁粗化优化前将这些语句挪到(即指令重排序)后一个同步块的临界区之中(当然, JIT编译器并不会将临界区内的代码挪到临界区之外)。 

在实际中，很小会出现上面例子的情况,下面的例子
```java

	public class Demo  {

		private final Random rnd = new Random();

		public void simulate(){
			int iq1 = randomIQ(); 
			int iq2 = randomIQ();
			int iq3 = randomIQ();
			act(iq1,iq2,iq3);
		}

		public void act(int... num) {

		}

		// 返回随机数
		public int randomIQ(){
			// rnd.nexGaussian() 是一个同步方法
			return (int)Math.round(rnd.nextGaussian() ＊ 15 + 100);
		}

	}
```
　　如上: simulate方法连续调用randomIQ方法来生成3个随机数, 在simulate方法被执行得足够频繁的情况下, JIT编译器可能对该方法执行一系优化：首先, JIT编译器可能将randomIQ方法内联(inline)到simulate方法中, 这相当于把randomIQ方法体中的指令复制到simulate方法之中。 这相当于把randomIQ方法体中的指令复制到simulate方法之中。 在此基础上, randomIQ方法中的rnd.nextGaussian()调用也可能被内联, 这相当于把Random.nextGaussian()方法体中的指令复制到simulate方法之中。 Random.nextGaussian()是一个同步方法, 由于Random实例rnd可能被多个线程共享(因为simulate方法可能被多个线程执行), 因此JIT编译器无法对Random.nextGaussian()方法本身执行锁消除优化, 这使得被内联到simulate方法中的Random.nextGaussian()方法体相当于一个由rnd引导的同步块。 

锁粗化默认是开启的。 如果要关闭这个特性, 我们可以在Java程序的启动命令行中添加虚拟机参数"-XX:-EliminateLocks"(开启则可以使用虚拟机参数"-XX:+EliminateLocks")。 

(3) 偏向锁
　　偏向锁(Biased Locking)是Java虚拟机对锁的实现所做的一种优化。 这种优化基于这样的观测结果(Observation)：大多数锁并没有被争用(Contented), 并且这些锁在其整个生命周期内至多只会被一个线程持有。 然而, Java虚拟机在实现monitorenter字节码(申请锁)和monitorexit字节码(释放锁)时需要借助一个原子操作(CAS操作), 这个操作代价相对来说比较昂贵。 因此, Java虚拟机会为每个对象维护一个偏好(Bias), 即一个对象对应的内部锁第1次被一个线程获得, 那么这个线程就会被记录为该对象的偏好线程(BiasedThread)。 这个线程后续无论是再次申请该锁还是释放该锁, 都无须借助原先(指未实施偏向锁优化前)昂贵的原子操作, 从而减少了锁的申请与释放的开销。 

　　然而, 一个锁没有被争用并不代表仅仅只有一个线程访问该锁, 当一个对象的偏好线程以外的其他线程申请该对象的内部锁时, Java虚拟机需要收回(Revoke)该对象对原偏好线程的"偏好"并重新设置该对象的偏好线程。 这个偏好收回和重新分配过程的代价也是比较昂贵的, 因此如果程序运行过程中存在比较多的锁争用的情况, 那么这种偏好收回和重新分配的代价便会被放大。 有鉴于此, 偏向锁优化只适合于存在相当大一部分锁并没有被争用的系统之中。

偏向锁优化默认是开启的。 要关闭偏向锁优化, 我们可以在Java程序的启动命令行中添加虚拟机参数"-XX:-UseBiasedLocking"(开启偏向锁优化可以使用虚拟机参数"-XX:+UseBiasedLocking")。 

(４) 适应性锁
适应性锁(AdaptiveLocking, 也被称为AdaptiveSpinning)是JIT编译器对内部锁实现所做的一种优化。 
　　存在锁争用的情况下, 一个线程申请一个锁的时候如果这个锁恰好被其他线程持有, 那么这个线程就需要等待该锁被其持有线程释放。 实现这种等待的一种保守方法：将这个线程暂停(线程的生命周期状态变为非Runnable状态)。 由于暂停线程会导致上下文切换, 因此对于一个具体锁实例来说, 这种实现策略比较适合于系统中绝大多数线程对该锁的持有时间较长的场景, 这样才能够抵消上下文切换的开销。 另外一种实现方法就是采用忙等(BusyWait)。就如一个空的循环` while(lockIsHeldByOtherThread) {}`

　　可见, 忙等是通过反复执行空操作(什么也不做)直到所需的条件成立为止而实现等待的。 这种策略的好处是不会导致上下文切换, 缺点是比较耗费处理器资源——如果所需的条件在相当长时间内未能成立, 那么忙等的循环就会一直被执行。 因此, 对于一个具体的锁实例来说, 忙等策略比较适合于绝大多数线程对该锁的持有时间较短的场景, 这样能够避免过多的处理器时间开销。

　　事实上, Java虚拟机也不是非要在上述两种实现策略之中择其一——它可以综合使用上述两种策略。 对于一个具体的锁实例, Java虚拟机会根据其运行过程中收集到的信息来判断这个锁是属于被线程持有时间"较长"的还是"较短"的。 对于被线程持有时间"较长"的锁, Java虚拟机会选用暂停等待策略; 而对于被线程持有时间"较短"的锁, Java虚拟机会选用忙等等待策略。 Java虚拟机也可能先采用忙等等待策略, 在忙等失败的情况下再采用暂停等待策略[7]。 Java虚拟机的这种优化就被称为适应性锁(AdaptiveLocking), 这种优化同样也需要JIT编译器介入。  

　　从适应性锁优化可以看出, 内部锁的使用并不一定会导致上下文切换

#### 优化对锁的使用

(1) 锁的开销
>1. 上下文切换与线程调度开销。 一个线程申请一个锁的时候, 如果这个锁恰好被其他线程持有, 那么该线程最终可能会被暂停。 Java虚拟机还需要为这些被暂停的线程维护一个等待队列(等待集), 以便在这个锁被其持有线程释放的时候将这些线程唤醒。 而线程的暂停与唤醒就是一个上下文切换的过程, 并且Java虚拟机维护等待队列也会产生一定的开销。 显然, 非争用锁并不会导致上下文切换和等待队列的开销。 
>2. 内存同步、编译器优化受限的开销。 锁的内部实现所使用的内存屏障也会产生直接和间接的开销：直接的开销是内存屏障所导致的冲刷写缓冲器、清空无效化队列所导致的开销。 另外, 内存屏障会阻碍某些编译器优化。 无论是争用锁还是非争用锁, 都会产生这部分开销[8]。 当然, 非争用的锁如果最终适用锁消除优化的话, 那么这个锁的任何开销都会被彻底消除。 
>3. 限制可伸缩性。 伸缩性。 锁的排他性的本质是局部地将并发计算改为串行计算。 这种特性会限制系统的可伸缩性。 假设系统的某个操作每次执行的时候都需要申请一个锁, 该锁平均被持有的时间为5毫秒, 那么1秒之内该系统最多只能完成200个这样的操作, 即这个系统的该操作的吞吐率为200TPS(TransactionperSecond), 无论这个系统有多少个处理器。 可见, 锁的排他性会导致处理器资源(以及其他资源)的浪费, 并限制系统的吞吐率。 

　　可见, 锁的开销主要体现在争用锁(ContentedLock)上面。 因此, 减少锁的开销的一个基本思路就是消除锁的使用(使用锁的替代品)或者降低锁的争用程度。 

　　影响锁的争用程度的因素有两个：程序申请锁的频率以及锁通常被持有的时间跨度。 程序越是频繁地申请一个锁, 或者这个锁通常被其持有线程持有的时间越长, 那么这个锁的争用程度就越高; 反之则该锁的争用程度就越低。 

(2) 使用可参数化锁
　　如果一个方法或者类内部锁使用的锁实例可以由该方法、类的客户端代码指定, 那么我们就称这个锁是可参数化的, 相应地, 这个锁就被称为可参数化的锁。 可参数化的锁在特定情况下有助于减少线程执行过程中参与的锁实例的个数, 从而减少锁的开销。 

　　Java标准库中的抽象类java.io.Writer就使用了可参数化的锁。 Writer类内部维护了一个protected修饰的实例变量lock, 该变量充当了Writer.write/flush/close等方法所需的内部锁。 Writer类默认使用其子类的当前实例(this关键字所代表的对象)作为lock的值, 即Writer类默认使用的锁实例是其子类的当前实例。 Writer类的子类可以通过在其构造器中调用Writer的构造器Writer(Objectlock)时指定一个对象或者直接在其构造器中为lock变量赋值的方式来设置Writer.write/flush/close等方法实际使用的锁实例。 

(3) 减小临界区的长度
　　减小临界区的长度可以减少锁被持有的时间从而降低锁被争用的概率, 这有利于减少锁的开销。 另外, 减少锁的持有时间有利于Java虚拟机的适用性锁优化发挥作用：在多数线程持有锁的时间都很短的情况下, 锁的申请线程可以通过忙等而无须通过暂停线程来等待被争用的锁的释放, 这有利于减少上下文切换开销。 

　　临界区逻辑上连贯的一些操作往往可以划分为几个部分：预处理操作(Pre-process)、共享变量访问操作以及后处理操作(Post-process)。 其中, 预处理操作和后处理操作往往是不涉及共享变量的访问的, 因此把这两种操作挪到临界区之外可以在不导致线程安全问题的前提下减小临界区的长度。 如果预处理操作、后处理操作中涉及I/O操作、阻塞操作等比较耗时的操作, 那么将这些操作挪到临界区之外可以有效地减少锁被持有的时间。 

(4) 减小锁的粒度
　　降低锁的争用程度的另外一种思路是降低锁的申请频率。 而减小锁的粒度可以降低锁的申请频率, 从而减小锁被争用的概率。 减小锁粒度的一种常见方法是将一个粒度较粗的锁拆分成若干粒度更细的锁, 其中每个锁仅负责保护(Guard)原粗粒度锁所保护的所有共享变量中的一部分共享变量。
![Alt '锁粒度'](https://github.com/LCN29/Picture-Repository/blob/master/MyNote/Book/lock-particle.png?raw=true)

　　锁拆分这种技术可以演进为另外一种被称为锁分段的技术。 锁分段(LockStriping)是指对同一个数据结构内不同部分的数据使用不同锁实例进行加锁的技术。 ConcurrentHashMap内部就使用了锁分段技术。 ConcurrentHashMap内部会创建N(默认值为16)个锁实例。 以put操作为例, 一个线程执行put方法时提供的key参数对应的HashCode(即key.hashCode()返回值)会传递给一个Hash函数, 该函数的返回值介于0与N-1之间。 ConcurrentHashMap通过该Hash函数的返回值就能够确定当前线程需要使用的锁实例。 因此, 同时执行put操作的不同线程只要其提供的key值不一样, 那么它们所需要使用的锁实例也可能是不一样的。 这就使得一个锁实例可以保护多个桶(Bucket)中的条目。 

　　锁分段会使对整个对象进行加锁变得困难甚至于不可能。 例如, 要对整个HashTable实例进行加锁, 我们只需要使用"synchronized(HashTable实例)"这样的语句, 而同样的方法无法实现在ConcurrentHashMap外部对整个ConcurrentHashMap实例进行加锁。 即使是在ConcurrentHashMap内部, 如果要实现对整个实例进行加锁的效果, 那么由于Java语言本身并不支持申请多个数量可变的锁(即一个线程申请N个锁, 而N的值在编译阶段不可知), 因此我们需要通过递归算法才能够实现

(5) 使用锁的替代品
在条件允许的情况下, 我们也可以考虑使用锁的替代品来避免锁的开销和问题。 这些有条件替代品包括：volatile关键字、原子变量、无状态对象、不可变对象和线程特有对象。


#### 减少系统内耗：上下文切换
　　在处理器个数远小于系统所需要支持的并发线程数的情况下, 上下文切换是保障处理器资源对各个线程能够"雨露均沾"的必要手段。 另外, 系统在一段时间内产生的上下文切换次数越多, 那么由此产生的处理器资源消耗也越多, 从而导致这段时间内真正能够用于执行应用代码的处理器资源越少。 上下文切换之于处理器就好比沟通之于团队, 团队协作中人员之间的沟通是必需的, 但是用于沟通的时间越多则真正能够用于办事的时间就越少。 

　　由于锁的争用会导致上下文切换, 因此减少锁的争用或者避免锁的使用都可以减少上下文切换。 另外, 我们也可以从以下几个方面入手来减少上下文切换。 
>1. 控制线程数量。 
>2. 避免在临界区中执行阻塞式I/O(BlockingI/O)等阻塞操作。 阻塞操作本身会导致上下文切换。 当一个线程因执行临界区中的阻塞操作而被暂停的时候, 这个线程所持有的引导该临界区的锁并没有被释放。 这就可能导致其他线程申请这个锁的时候, 该锁仍然还被这个被暂停的线程所持有。 因此, 临界区中的阻塞操作会增加引导这个临界区的锁被争用的可能性。 而被争用的锁又可能导致上下文切换, 因此在临界区中执行阻塞操作会进一步增加上下文切换。 避免在临界区中执行阻塞式I/O等阻塞操作的典型技巧是在多线程环境中特意使用单线程来执行I/O操作。 
>3. 避免在临界区中执行比较耗时的操作。 在临界区中执行比较耗时的操作会增加引导该临界区的锁的持有时间, 从而增加这个锁被争用的概率。 而被争用的锁可能导致上下文切换。 因此, 在临界区中执行比较耗时的操作也会增加上下文切换的可能性。 
>4. 减少Java虚拟机的垃圾回收。 Java垃圾回收器的运行可能导致Stop-the-World事件, 即所有应用线程被暂停的现象。 Java垃圾回收器(GarbageCollector)在其工作过程中往往需要移动存活对象(LiveObject, 即未被垃圾回收掉的对象)。 

#### 性能杀手：伪共享
　　由于一个缓存行中可以存储多个变量的副本, 因此即便是在两个线程各自仅访问各自的共享变量(它们之间不存在共同的共享变量)的情况下, 一个线程更新其共享变量也可能导致另外一个线程访问其共享变量时产生缓存未命中, 这种现象就被称为伪共享(False Sharing)。 

　　在多个线程访问同一组共享变量的情况下, 一个处理器上的线程更新了其中一个共享变量, 会导致其他处理器上包含这个共享变量副本的缓存条目被无效化(Invalidated), 即相应缓存条目的状态被置为I; 因此, 这些处理器上运行的其他线程再次访问(包括读和写)这个被无效化的缓存条目的缓存行中曾经存有副本的任何一个共享变量时, 都会产生缓存未命中(CacheMiss)。 

(1)Java对象内存布局
　　描述一个Java对象在堆内存内存(Heap Memory)中的存储形式。

　　Java对象在内存中的存储包括对象头(Object Header)和实例字段。 其中, 对象头会使用2个字(Word, 计算机进行数据处理时，一次存取、加工和传送的数据长度称为字)的存储空间：第1个字用于存储对象的HashCode、锁的相关信息(比如偏向锁的偏向线程的ID)等信息; 第2个字用于存储对象所属类的指针。 因此对象头会占用8字节(32位处理器下)或者16字节(64位处理器下)。 另外, 如果对象是一个数组, 那么Java虚拟机会使用额外的一个字来表示数组的长度, 即数组的对象头会占用3个字的空间。 为了节约空间, 在4字节足以表示对象所属类的地址的情况下, Java虚拟机会仅使用4字节来表示对象头中的第2个字。 因此, 在64位系统下对象头可能只占用12(8 + 4)字节。 `1个字节 = 8 位, 但是不同的计算机下, 存在 1个word = 不同的字节, 64位的通常是 1个word = 8个字节 = 64位`

　　总的来说, Java虚拟机会为待创建的对象分配一段存储空间。 这段存储空间的起始位置处(位置偏移为0)存储的是对象头, 对象头占用的空间之后存储的是对象的各个实例字段。 当然, 实例字段的数量可能是0(无状态对象)也可能是多个。 为了提高内存访问的效率并减少由此导致的内存空间的浪费, Java虚拟机会依照一定的规则将一个对象的对象头及该对象包含的实例字段分配到内存空间中进行存储。

>1. 规则1 对象是以8字节为粒度(Granularity)进行对齐(Aligned)的。 
　　这个规则也被称为对象是8字节对齐(8-Bytes Aligned)进行存储的。 所谓的8字节对齐, 可以这样理解：把内存空间看成一个个"小格子", 其中每个小格子的容量是8字节。 如图, 假设我们有4个数据a(占用4字节)、b(占用4字节)、c(占用4字节)和d(占用8字节)要存入内存, 并且这些数据要按照它们所占用的空间大小的顺序进行存储[16]。 那么, 我们可以先将a存入第1个小格子中, 此时这个小格子还剩余的4字节的空间可以用来存放b。 然后, 我们将c存入第2个小格子。 这时我们只剩下d待存储, 但是此时第2个小格子还剩的4字节的空间无法用来存储需要8字节空间的d。 因此, 我们先将第2个小格子填满, 填满的方法就是往其中存储一个占用4字节的填充材料(Padding)。 接着, 我们便可以将d存入第3个格子。

![Alt '8位对齐'](https://github.com/LCN29/Picture-Repository/blob/5828f01fa897bff6e98172d4ab8b2beaf1914ae4/MyNote/Book/8-bytes-aligned.png?raw=true)

　　可见, 所谓"对齐"往往需要"填充", 而"填充"会带来一定的存储空间浪费。 为了尽量减少这种浪费, Java虚拟机在存储实例变量的时候并不是依照源代码中的声明顺序而是依照实例变量所占用的空间大小顺序进行。 在Java平台中, boolean/byte型变量占用1字节的空间, short/char型变量占用2字节的空间, int/float型变量占用4字节的空间, long/double型变量占用8字节的空间, 引用型变量占用4字节(32位系统)或者8字节(64位系统, 且不开启指针压缩)的空间。 （由此得出第二条规则）

>2. 规则2对象中的实例字段按照如下顺序而非其源代码声明顺序排列。 
>>1. long型变量和double型变量
>>2. int型变量和float型变量
>>3. short型变量和char型变量
>>4. boolean型变量和byte型变量
>>5. 引用型变量

　　规则2在某些情形下出于节约空间的需要也可能被打破。 如类继承时(由此得出第三条规则)。

>3. 继承自父类的实例字段不会与类本身定义的实例字段混杂在一起进行存储。 

其实还有其他的一些规则,在平时可以借助OpenJDK项目下的工具jol(Java Object Layout)来查看对象的实际内存布局。

(2) 伪共享的侦测与消除
　　消除伪共享的一个方法就是填充(Padding)。 由于伪共享产生的前提是多个线程访问了位于同一缓存行(不同的处理器的缓存行大小时不一样的)之中的共享变量(尽管这些线程并没有访问同一个共享变量), 因此消除伪共享的一个直观思路就是设法不让这些线程所访问的共享变量被加载到同一个缓存行之中。 填充就是通过在类中添加一些"无用的"(没有功能上的用途)实例变量来"干扰"对象的内存布局, 以使特定的实例变量(或者某个实例)能够独自占用一个缓存行的空间, 从而避免这些实例变量(或者实例)与其他实例变量(或者实例)被加载到同一个缓存行之中。 

填充带来的问题
>1. 填充虽然能够在不改变程序算法的情况下使得程序的性能有显著的提升, 但是, 填充显然是一种以空间换时间的优化手段, 因此大规模地使用填充可能导致过多的额外空间消耗, 从而增加垃圾回收器的负担。 
>2. 要正确地实现填充, 我们必须需要知道系统的缓存行宽度, 还要了解和确定Java对象的内存布局。 然而, Java语言本身并没有提供用于获取缓存行宽度的接口, 并且不同处理器的缓存行宽度也可能不一样(从16字节到128字节不等)。 因此, 对缓存行宽度的依赖使得填充这种技术存在硬件层面的可移植性问题(更换或者升级机器)。 填充时具体在字段声明的什么地方、填充多少个字节的内容实际上取决于对象的内存布局。 而要了解Java对象的内存布局无疑增加了对人员的要求, 并且由于不同的Java虚拟机可能有不同的内存布局规则, 因此, 对Java对象内存布局的依赖同样也使得填充这种技术存在软件层面的可移植性问题(部署在Java虚拟机上, 或者切换Java虚拟机)。 
>3. 直接在Java源码上进行填充, Java虚拟机可能会将"无用的"字段给优化掉。 但是我们却使用protected(而不是private)和volatile来修饰这些变量以达到"欺骗"Java虚拟机的目的——这些变量是有可能被当前线程或者其他线程访问的, 否则Java虚拟机会认为这些字段是"无用的"而将其优化掉。 
另外要填充比较大的空间, 比如需要填充128字节, 那么我们需要用"protectedvolatilelong[]paddings=newlong[128]"这样的填充方式。 

　　在上面说的，直接在源代码进行填充的，叫做手动填充。在Java8引入了一个特殊的注解@sun.misc.Contended, 该注解可以用来注释字段和类, 它的作用是给Java虚拟机一个提示——被注释的字段(仅限实例变量)或者类的实例可能面临伪共享问题。 Java虚拟机则根据这个注解进行填充来使得被注释的实例变量或者类的实例能够被加载到单独的一个缓存行之中。 因此, 这种填充被称为自动填充。 
```java

   @sun.misc.Contended
   public volatile long value;

```

　　由于目前默认情况下@sun.misc.Contended仅开放给JDK内部的类, 因此, 应用自身的类要使用该注解时需要开启Java虚拟机的开关"-XX:-RestrictContended"。 

　　自动填充避免了手动填充存在的一些问题(可移植性等问题)和不便, 但是它比手动填充更耗空间 ——JDK1.8在@sun.misc.Contended注释的字段(或者类的实例)前和后各自填充大小为缓存行宽度的2倍的填充空间。 因此, 依照性能优化"避免过早优化"的原则, 我们应该只在确认存在伪共享问题的情况下才考虑使用填充。 


　　减少共享变量的访问频率有助于降低伪共享问题出现的频率。 